{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClimateWatch Data Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bkPGMr-JUkV"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnRM0BqhjgTM"
      },
      "source": [
        "from ipywidgets import interact, Dropdown\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn import model_selection as skms\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing as skpp\n",
        "\n",
        "import os\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "import traceback\n",
        "from tensorflow import keras\n",
        "\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from sklearn import datasets\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ3fghZPgjsi"
      },
      "source": [
        "# Green House Gases Emmision Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu3umzFGJThV"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AdEllTrJThV"
      },
      "source": [
        "# Climate Analysis Indicators Tool (CAIT) \n",
        "cait=r\"data\\CW_CAIT_GHG_Emissions.xlsx'\n",
        "cait = pd.ExcelFile(cait)\n",
        "# Potsdam Institute for Climate Impact Research (PIK) \n",
        "pik=r\"data\\CW_PIK_GHG_Emissions.xlsx'\n",
        "pik = pd.ExcelFile(pik)\n",
        "\n",
        "cait_data = {sheet_name: cait.parse(sheet_name) \n",
        "          for sheet_name in cait.sheet_names}\n",
        "cait_data = cait_data['CAIT_2019_update']\n",
        "cait_data.columns=[str(j) for j in cait_data.columns]\n",
        "cait_data.columns = cait_data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "pik_data = {sheet_name: pik.parse(sheet_name) \n",
        "          for sheet_name in pik.sheet_names}\n",
        "pik_data=pik_data['CW_HistoricalEmissions_PIK_2019']\n",
        "pik_data.columns=[str(j) for j in pik_data.columns]\n",
        "pik_data.columns = pik_data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3-tKRL0KHRH"
      },
      "source": [
        "## Univariate MLP Timeseries Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LjsPe2AJThV"
      },
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        end_ix = i + n_steps\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "\n",
        "def keras_predict(listt, x_input):\n",
        "    raw_seq=listt\n",
        "    n_steps = 3\n",
        "    X, y = split_sequence(raw_seq, n_steps)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, activation='relu', input_dim=n_steps))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X, y, epochs=2000, verbose=0)\n",
        "    x_input = array(x_input)\n",
        "    x_input = x_input.reshape((1, n_steps))\n",
        "    yhat = model.predict(x_input, verbose=0)\n",
        "    return yhat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIe9Dy13KMlS"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9ed4d306437b4ace848383da16efbe93"
          ]
        },
        "id": "v0iRzILuJThW",
        "outputId": "0d6e401f-53ef-4465-9d6f-11235ac862af"
      },
      "source": [
        "dataaw = Dropdown(options = ['CAIT', 'PIK'])\n",
        "\n",
        "@interact(file = dataaw)\n",
        "def gas_data(file):  \n",
        "    if (file == 'CAIT'):\n",
        "        dataa = cait_data.copy()\n",
        "    else:\n",
        "        dataa = pik_data.copy()    \n",
        "    country_drop = Dropdown(options = list(set(list(dataa['country']))))\n",
        "    sector_drop = Dropdown(options = list(set(list(dataa['sector']))))\n",
        "    gas_drop = Dropdown(options = list(set(list(dataa['gas']))))\n",
        "    @interact(country_name=country_drop, sec_name = sector_drop, gas_name=gas_drop)\n",
        "    def print_city(country_name, sec_name, gas_name):\n",
        "        print('country_name:',country_name , sep=' ')\n",
        "        print( 'sec_name:',sec_name , sep=' ')\n",
        "        print( 'gas_name:',gas_name,  sep=' ')\n",
        "        try:\n",
        "            ts_data=dataa[(dataa['country']==country_name) & (dataa['sector']==sec_name) & (dataa['gas']==gas_name)].transpose()[4:]\n",
        "            ts_data['year']=ts_data.index\n",
        "            ts_data.reset_index(drop=True, inplace=True)\n",
        "            ts_data.columns=['value', 'year']\n",
        "            ts_data.dropna(subset = [\"value\"], inplace=True)\n",
        "            print('TimeSeries  Data:=')\n",
        "            print(ts_data.transpose())\n",
        "            values_list = list(ts_data['value'])\n",
        "            print('Next Value could be', \n",
        "                  keras_predict(values_list,values_list[-3:] ))\n",
        "        except:\n",
        "            print('Data Not Available')\n",
        "            return False\n",
        "#         ts_data=dataa[dataa['']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ed4d306437b4ace848383da16efbe93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='file', options=('CAIT', 'PIK'), value='CAIT'), Output()), _dom_claâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQ2FimUKTRF"
      },
      "source": [
        "## Simple Single Layer Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0DlDLjVJThX"
      },
      "source": [
        "def normalize_cols(m):\n",
        "    col_max = m.max(axis=0)\n",
        "    col_min = m.min(axis=0)\n",
        "    return (m-col_min) / (col_max - col_min)\n",
        "def single_neu_net(np_array):\n",
        "    x_vals = np.array([x[0:3] for x in np_array])\n",
        "    y_vals = np.array([x[3] for x in np_array])\n",
        "    sess = tf.Session()\n",
        "    seed = 2\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)  \n",
        "    train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\n",
        "    test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
        "    x_vals_train = x_vals[train_indices]\n",
        "    x_vals_test = x_vals[test_indices]\n",
        "    y_vals_train = y_vals[train_indices]\n",
        "    y_vals_test = y_vals[test_indices]\n",
        "    x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\n",
        "    x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))\n",
        "    batch_size = 50\n",
        "    x_data = tf.placeholder(shape=[None, 3], dtype=tf.float32)\n",
        "    y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "    y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "    hidden_layer_nodes = 10\n",
        "    A1 = tf.Variable(tf.random_normal(shape=[3,hidden_layer_nodes])) # inputs -> hidden nodes\n",
        "    b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))   # one biases for each hidden node\n",
        "    A2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,1])) # hidden inputs -> 1 output\n",
        "    b2 = tf.Variable(tf.random_normal(shape=[1]))   # 1 bias for the output\n",
        "    hidden_output = tf.nn.relu(tf.add(tf.matmul(x_data, A1), b1))\n",
        "    final_output = tf.nn.relu(tf.add(tf.matmul(hidden_output, A2), b2))\n",
        "    loss = tf.reduce_mean(tf.square(y_target - final_output))\n",
        "    my_opt = tf.train.GradientDescentOptimizer(0.005)\n",
        "    train_step = my_opt.minimize(loss)\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    loss_vec = []\n",
        "    test_loss = []\n",
        "    for i in range(500):\n",
        "        rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
        "        rand_x = x_vals_train[rand_index]\n",
        "        rand_y = np.transpose([y_vals_train[rand_index]])\n",
        "        sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
        "\n",
        "        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
        "        loss_vec.append(np.sqrt(temp_loss))\n",
        "\n",
        "        test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n",
        "        test_loss.append(np.sqrt(test_temp_loss))\n",
        "        if (i+1)%50==0:\n",
        "            print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n",
        "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "    plt.plot(loss_vec, 'k-', label='Train Loss')\n",
        "    plt.plot(test_loss, 'r--', label='Test Loss')\n",
        "    plt.title('Loss (MSE) per Generation')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel('Generation')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyx4tGXMKcQF"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "16f5269e1a3a4ed18ff588ca7bc2902e"
          ]
        },
        "id": "YrivNiGOJThY",
        "outputId": "d2007d60-b16f-4f10-f48a-4065c38bf310"
      },
      "source": [
        "dataaw = Dropdown(options = ['CAIT', 'PIK'])\n",
        "\n",
        "@interact(file = dataaw)\n",
        "def gas_data(file):  \n",
        "    if (file == 'CAIT'):\n",
        "        dataa = cait_data.copy()\n",
        "    else:\n",
        "        dataa = pik_data.copy()\n",
        "#     print(dataa)\n",
        "    sector_drop = Dropdown(options = list(set(list(dataa['sector']))))\n",
        "    gas_drop = Dropdown(options = list(set(list(dataa['gas']))))\n",
        "    year_drop = Dropdown(options = list(dataa.columns[4:]))\n",
        "    @interact(year=year_drop, sec_name = sector_drop, gas_name=gas_drop)\n",
        "    def print_city(year, sec_name, gas_name):\n",
        "        print('year:',year , sep=' ')\n",
        "        print( 'sec_name:',sec_name , sep=' ')\n",
        "        print( 'gas_name:',gas_name,  sep=' ')\n",
        "\n",
        "        try:\n",
        "            cols=['country', 'sector', 'gas' ]\n",
        "            for j in cols:\n",
        "                cons=list(set(list(dataa[j])))\n",
        "                con_dict={cons[i]:i for i in range(len(cons))}\n",
        "                dataa[j]=[con_dict[i] for i in dataa[j]]\n",
        "\n",
        "            new_data = dataa[[ 'gas', 'sector',year, 'country',]].copy()\n",
        "            print(new_data)\n",
        "            single_neu_net(new_data.to_numpy())\n",
        "        except:\n",
        "            print('Data Not Available')\n",
        "            return False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16f5269e1a3a4ed18ff588ca7bc2902e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='file', options=('CAIT', 'PIK'), value='CAIT'), Output()), _dom_claâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdCQZ_abJThY"
      },
      "source": [
        "# CW_WB_Climate_Vulnerability_Readiness Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPGz1R_YKkHz"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mGnUR-UJThZ",
        "outputId": "cf536468-406a-4705-9945-3418b77c6d88"
      },
      "source": [
        "xl_file = pd.ExcelFile(r\"data\\CW_WB_Climate_Vulnerability_Readiness_31102017.xlsx\")\n",
        "\n",
        "dfs = {sheet_name: xl_file.parse(sheet_name) \n",
        "          for sheet_name in xl_file.sheet_names}\n",
        "dataa=dfs['Vulnerability_Readiness']\n",
        "col_list=['country',\n",
        "          'Population living below the national income poverty line',\n",
        "          'Climate Risk Score',\n",
        "          'Vulnerability score',\n",
        "          'Readiness score',\n",
        "          'Link to climate risk and adaptation profile',\n",
        "          'Droughts',\n",
        "          'Storms',\n",
        "          'Epidemics',\n",
        "          'Landslides',\n",
        "          'Floods',\n",
        "          'Wildfires'\n",
        "]\n",
        "\n",
        "dataa.columns=col_list\n",
        "dataa.columns=[str(j) for j in dataa.columns]\n",
        "dataa.columns = dataa.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "dataa.dropna(subset = [\"country\"], inplace=True)\n",
        "dataa.dropna(subset = [\"readiness_score\"], inplace=True)\n",
        "dataa=dataa.drop([\"link_to_climate_risk_and_adaptation_profile\"], axis=1)\n",
        "dataa=dataa[3:].copy().reset_index().drop(['index'], axis=1)\n",
        "for i in dataa.columns[5:]:\n",
        "    dataa[i] = dataa[i].fillna(-1)\n",
        "    dataa[i] = dataa[i].replace(['YES'],1)\n",
        "    dataa[i] = dataa[i].replace(['NO'],-1)\n",
        "con_dict = {dataa['country'][i]:i for i in range(len(dataa['country']))}\n",
        "print(\"Data Preview:-\")\n",
        "print(dataa.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preview:-\n",
            "  country population_living_below_the_national_income_poverty_line  \\\n",
            "0     AGO                                                NaN         \n",
            "1     ALB                                               14.3         \n",
            "2     ARE                                                NaN         \n",
            "3     ARG                                                NaN         \n",
            "4     ARM                                                 30         \n",
            "\n",
            "  climate_risk_score vulnerability_score readiness_score  droughts  storms  \\\n",
            "0             106.67            0.566557        0.289926        -1      -1   \n",
            "1              125.5            0.386407        0.525043        -1      -1   \n",
            "2             154.67            0.364053        0.603236        -1      -1   \n",
            "3              84.67             0.37448        0.440038        -1      -1   \n",
            "4              139.5            0.384046        0.537698        -1      -1   \n",
            "\n",
            "   epidemics  landslides  floods  wildfires  \n",
            "0         -1          -1      -1         -1  \n",
            "1         -1          -1      -1         -1  \n",
            "2         -1          -1      -1         -1  \n",
            "3         -1          -1      -1         -1  \n",
            "4         -1          -1      -1         -1  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqIAft2Km6L"
      },
      "source": [
        "## Gaussion SVM Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X62hmoAhJThZ"
      },
      "source": [
        "def gaus_svm(tonumpy_data, data_ori, features):\n",
        "    dataa = tonumpy_data\n",
        "    x_vals = np.array([[x[0], x[1]] for x in dataa])\n",
        "    y_vals = np.array(list(data_ori[features[2]]))\n",
        "    class1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]\n",
        "    class1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]\n",
        "    class2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]\n",
        "    class2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]\n",
        "    sess = tf.Session()\n",
        "    batch_size = len(x_vals)\n",
        "    x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
        "    y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "    prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
        "    b = tf.Variable(tf.random_normal(shape=[1,batch_size]))\n",
        "    gamma = tf.constant(-50.0)\n",
        "    sq_vec = tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))\n",
        "    my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_vec)))\n",
        "    first_term = tf.reduce_sum(b)\n",
        "    b_vec_cross = tf.matmul(tf.transpose(b), b)\n",
        "    y_target_cross = tf.matmul(y_target, tf.transpose(y_target))\n",
        "    second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))\n",
        "    loss = tf.negative(tf.subtract(first_term, second_term))\n",
        "    rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])\n",
        "    rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\n",
        "    pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
        "    pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n",
        "    prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b), pred_kernel)\n",
        "    prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))\n",
        "    my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
        "    train_step = my_opt.minimize(loss)\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    loss_vec = []\n",
        "    batch_accuracy = []\n",
        "    for i in range(300):\n",
        "        rand_index = np.random.choice(len(x_vals), size=batch_size)\n",
        "        rand_x = x_vals[rand_index]\n",
        "        rand_y = np.transpose([y_vals[rand_index]])\n",
        "        sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
        "\n",
        "        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
        "        loss_vec.append(temp_loss)\n",
        "\n",
        "        acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x,\n",
        "                                                 y_target: rand_y,\n",
        "                                                 prediction_grid:rand_x})\n",
        "        batch_accuracy.append(acc_temp)\n",
        "\n",
        "        if (i+1)%75==0:\n",
        "            print('Step #' + str(i+1))\n",
        "            print('Loss = ' + str(temp_loss))\n",
        "    x_min, x_max = x_vals[:, 0].min() - 1, x_vals[:, 0].max() + 1\n",
        "    y_min, y_max = x_vals[:, 1].min() - 1, x_vals[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    [grid_predictions] = sess.run(prediction, feed_dict={x_data: x_vals,\n",
        "                                                       y_target: np.transpose([y_vals]),\n",
        "                                                       prediction_grid: grid_points})\n",
        "    grid_predictions = grid_predictions.reshape(xx.shape)\n",
        "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "    plt.contourf(xx, yy, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
        "    plt.plot(class1_x, class1_y, 'ro', label='Immune')\n",
        "    plt.plot(class2_x, class2_y, 'kx', label='Not Immune')\n",
        "    plt.title('Gaussian SVM Results on Climate_Vulnerability_Readiness Data for '+features[2])\n",
        "    plt.xlabel(features[0])\n",
        "    plt.ylabel(features[1])\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "    plt.plot(batch_accuracy, 'k-', label='Accuracy')\n",
        "    plt.title('Batch Accuracy')\n",
        "    plt.xlabel('Generation')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "    plt.plot(loss_vec, 'k-')\n",
        "    plt.title('Loss per Generation')\n",
        "    plt.xlabel('Generation')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FtVVuOeKs9L"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ba0372f6ffaa460f9baa4c0443b339ea"
          ]
        },
        "id": "5quyCMXkJTha",
        "outputId": "e3f43c26-a964-4da9-cb44-7ca26edf48b8"
      },
      "source": [
        "Feature1 = Dropdown(options = ['climate_risk_score','vulnerability_score','readiness_score'])\n",
        "Feature2 = Dropdown(options = ['climate_risk_score','vulnerability_score','readiness_score'])\n",
        "Calamity = Dropdown(options = ['droughts','storms','epidemics','landslides','floods','wildfires'])\n",
        "@interact(f1=Feature1, f2=Feature2, cal=Calamity)\n",
        "def clim_vul_data(f1, f2, cal):\n",
        "    if (f1==f2):\n",
        "        print('same Feature Selected, Please Select Different one')\n",
        "        return False\n",
        "    new_data = dataa.filter([f1,f2,cal], axis=1)\n",
        "    for i in new_data.columns:\n",
        "        new_data[i] = new_data[i].fillna(0) \n",
        "    gaus_svm(new_data.to_numpy(), new_data, [f1,f2,cal])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba0372f6ffaa460f9baa4c0443b339ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='f1', options=('climate_risk_score', 'vulnerability_score', 'readinâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xo9mZrzJThb"
      },
      "source": [
        "# PathWays Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qujzxowKwGL"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N_LiYPcJThb"
      },
      "source": [
        "gcam = pd.ExcelFile(r\"data\\GCAM.xlsx\")\n",
        "gcam = {sheet_name: gcam.parse(sheet_name) \n",
        "          for sheet_name in gcam.sheet_names}\n",
        "gcam = gcam['GCAM_Timeseries data']\n",
        "ieo = pd.ExcelFile(r\"data\\IEO.xlsx\")\n",
        "ieo = {sheet_name: ieo.parse(sheet_name) \n",
        "          for sheet_name in ieo.sheet_names}\n",
        "ieo = ieo['IEO_Timeseries data']\n",
        "\n",
        "df_list = [gcam, ieo]\n",
        "for dfs in df_list:\n",
        "    dfs.columns=[str(j) for j in dfs.columns]\n",
        "    dfs.columns = dfs.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70e7KkYLK1tv"
      },
      "source": [
        "## Multivariate MLP TimeSeries Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2IAmlkJThb"
      },
      "source": [
        "def split_sequences(sequences, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the dataset\n",
        "        if end_ix > len(sequences):\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "\n",
        "def multivar_timeseries(listt):\n",
        "    list1=listt[0]\n",
        "    list2=listt[1]\n",
        "    in_seq1 = array(list1)\n",
        "    in_seq2 = array(list2)\n",
        "    out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
        "    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "    out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "    dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "    n_steps = 3\n",
        "    X, y = split_sequences(dataset, n_steps)\n",
        "    n_input = X.shape[1] * X.shape[2]\n",
        "    X = X.reshape((X.shape[0], n_input))\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, activation='relu', input_dim=n_input))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X, y, epochs=2000, verbose=0)\n",
        "    # x_input = array([[80, 85], [90, 95], [100, 105]])\n",
        "    x_input = array([list1[-3:],list2[-3:]]).transpose()\n",
        "    x_input = x_input.reshape((1, n_input))\n",
        "    yhat = model.predict(x_input, verbose=0)\n",
        "    return yhat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qffQdVXK9I7"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "46274a6286094dc991a4a9b172dfc607"
          ]
        },
        "id": "buXz9UjJJThb",
        "outputId": "2a23e4da-7637-4992-9259-757bd5ddd2d3"
      },
      "source": [
        "typee = Dropdown(options = ['GCAM', 'IEO'])\n",
        "@interact(typee=typee)\n",
        "def clim_vul_data(typee):\n",
        "    dataa=globals()[typee.lower()]\n",
        "    dataa=dataa.fillna(0)\n",
        "    scenario = Dropdown(options = dataa.scenario.unique())\n",
        "    indi_name = Dropdown(options = dataa.esp_indicator_name.unique())\n",
        "    region1 = Dropdown(options = dataa.region.unique())\n",
        "    region2 = Dropdown(options = dataa.region.unique())\n",
        "    @interact(region1=region1, region2=region2, scenario=scenario, indi_name=indi_name)\n",
        "    def in_data(scenario, indi_name, region1, region2):\n",
        "        print('scenario:',scenario,'\\nesp_indicator_name:', indi_name,'\\nregion1:', region1,'\\nregion2:', region2)\n",
        "        print('Unit:', dataa[dataa['esp_indicator_name']==indi_name].unit_of_entry.unique())\n",
        "        r1_ts=dataa[(dataa['scenario']==scenario) & (dataa['esp_indicator_name']==indi_name) & (dataa['region'] == region1) ]\n",
        "        r2_ts=dataa[(dataa['scenario']==scenario) & (dataa['esp_indicator_name']==indi_name) & (dataa['region'] == region2) ]\n",
        "        reg_df=[r1_ts, r2_ts]\n",
        "        for i in range(len(reg_df)):\n",
        "            reg_df[i]=reg_df[i].transpose()[5:]\n",
        "            reg_df[i]['year']=reg_df[i].index\n",
        "            reg_df[i].reset_index()\n",
        "            reg_df[i].columns=['value', 'year']\n",
        "            print('Region', str(i+1), 'Series:=')\n",
        "            print(reg_df[i].transpose())\n",
        "        try:\n",
        "            print('Next Value for both could be', str(multivar_timeseries([list(i['value']) for i in reg_df])/2))\n",
        "        except:\n",
        "            print('Series Inconsistent!')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46274a6286094dc991a4a9b172dfc607",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='typee', options=('GCAM', 'IEO'), value='GCAM'), Output()), _dom_clâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J83AglrLJThb"
      },
      "source": [
        "# socioeconomic_indicators_GDP Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnBjMKYRLA2b"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3ZoT9CIJThc"
      },
      "source": [
        "soc_eco = pd.ExcelFile(r\"data\\socioeconomic_indicators_GDP.xlsx\")\n",
        "soc_eco = {sheet_name: soc_eco.parse(sheet_name) \n",
        "          for sheet_name in soc_eco.sheet_names}\n",
        "soc_eco = soc_eco['socioeconomic_indicators_GDP']\n",
        "col_list=[]\n",
        "for i in soc_eco.iloc[[1]].columns:\n",
        "    col_list.append(list(soc_eco.iloc[[1]][i])[0])\n",
        "col_list=[str(j) for j in col_list]\n",
        "col_list = [i.strip().lower().replace(' ', '_').replace('(', '').replace(')', '') for i in col_list]\n",
        "units=[]\n",
        "for i in soc_eco.iloc[[2]].columns:\n",
        "    units.append(list(soc_eco.iloc[[2]][i])[0])\n",
        "soc_eco.columns=col_list\n",
        "unit_dict=dict()\n",
        "for i in range(len(units)):\n",
        "    unit_dict[col_list[i]]=units[i] \n",
        "soc_eco=soc_eco[3:]\n",
        "soc_eco=soc_eco.reset_index()\n",
        "soc_eco=soc_eco.drop(['index'], axis = 1) \n",
        "soc_eco=soc_eco.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jth50IKuLIkr"
      },
      "source": [
        "## Simple KERAS Model Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "vD2_0aGvJThc",
        "outputId": "27268c55-42d1-4f82-fc06-13be0375dedc"
      },
      "source": [
        "year = Dropdown(options = ['2011', '2012','2013','2014','2015','2016'])\n",
        "asd=[]\n",
        "@interact(year=year)\n",
        "def soc_eco_fun(year):\n",
        "    res = [i for i in soc_eco.columns if year in i] \n",
        "    print(res)\n",
        "    columnss=list(soc_eco.columns[2:])\n",
        "    pred_ft = Dropdown(options = list([item for item in columnss if item not in res]))\n",
        "    @interact(pred_ft=pred_ft)\n",
        "    def pred_var(pred_ft):\n",
        "        print('Prediction Column:', pred_ft)\n",
        "        new_list=res+[pred_ft]\n",
        "        new_df=soc_eco[new_list].copy()\n",
        "        new_df[pred_ft] = new_df[pred_ft].astype('category')\n",
        "        new_df[pred_ft] = new_df[[pred_ft]].apply(lambda x: x.cat.codes)\n",
        "        features = new_df.drop(pred_ft, axis=1).to_numpy()\n",
        "        labels = new_df[pred_ft].to_numpy()\n",
        "        train_features = features[:-1, :]\n",
        "        test_features = features[-1:]\n",
        "        train_labels = labels[:-1]\n",
        "        test_labels = labels[-1:]\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(256, activation='relu'),\n",
        "            keras.layers.Dense(5, activation='softmax')\n",
        "        ])\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        model.fit(train_features, train_labels, epochs=15, validation_split=0.3)\n",
        "        print('Model Prediction : ',model.predict(test_features))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='year', options=('2011', '2012', '2013', '2014', '2015', '2016'), vâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZuODdk_JThc"
      },
      "source": [
        "# CW_Agriculture_Profile Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDWkyC6qLSW8"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxtIYFa1jqup",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "f075d809-2186-48b2-b9f5-985759407167"
      },
      "source": [
        "xl_file = pd.ExcelFile(r\"data\\CW_Agriculture_Profile.xlsx\")\n",
        "\n",
        "dfs = {sheet_name: xl_file.parse(sheet_name) \n",
        "          for sheet_name in xl_file.sheet_names}\n",
        "for i in dfs.keys():\n",
        "    dfs[i].columns=[str(j) for j in dfs[i].columns]\n",
        "    dfs[i].columns = dfs[i].columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "\n",
        "sheets=list(dfs.keys())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-85475707781f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxl_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"data\\CW_Agriculture_Profile.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m dfs = {sheet_name: xl_file.parse(sheet_name) \n\u001b[1;32m      4\u001b[0m           for sheet_name in xl_file.sheet_names}\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2k1p_xqLVbL"
      },
      "source": [
        "## Simple Tensorflow Regression Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9b4GJBfjqwp"
      },
      "source": [
        "#linear regression\n",
        "def tens_lin_reg(year, value):\n",
        "    x_vals = np.array([float(i) for i in year])\n",
        "    y_vals = np.array(value)\n",
        "    sess = tf.Session()\n",
        "    batch_size = 10\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    x_data = tf.compat.v1.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "    y_target = tf.compat.v1.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "    A = tf.Variable(tf.random.normal(shape=[1,1]))\n",
        "    b = tf.Variable(tf.random.normal(shape=[1,1]))\n",
        "    model_output = tf.add(tf.matmul(x_data, A), b)\n",
        "    loss = tf.reduce_mean(tf.square(y_target - model_output))\n",
        "    tf.disable_v2_behavior()\n",
        "    my_opt=tf.train.GradientDescentOptimizer(learning_rate=0.00000009)\n",
        "    train_step = my_opt.minimize(loss)\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    loss_vec = []\n",
        "    for i in range(500):\n",
        "        rand_index = np.random.choice(len(x_vals), size=batch_size)\n",
        "        rand_x = np.transpose([x_vals[rand_index]])\n",
        "        rand_y = np.transpose([y_vals[rand_index]])\n",
        "        _, temp_loss=sess.run([train_step,loss], feed_dict={x_data: rand_x, y_target: rand_y})\n",
        "        loss_vec.append(temp_loss)\n",
        "        if (i+1)%100==0:\n",
        "            print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n",
        "            print('Loss = ' + str(temp_loss))\n",
        "    [slope] = sess.run(A)\n",
        "    [y_intercept] = sess.run(b)\n",
        "    best_fit = []\n",
        "    for i in x_vals:\n",
        "      best_fit.append(slope*i+y_intercept)\n",
        "    fig, axs = plt.subplots(2)\n",
        "    axs[0].plot(x_vals, y_vals, 'o', label='Data Points')\n",
        "    axs[0].plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\n",
        "    axs[0].legend(loc='upper left')\n",
        "    axs[1].plot(loss_vec, 'k-', label='Loss Per Generation')\n",
        "    # axs[1].show()\n",
        "    fig.suptitle('Data Regression Plot')\n",
        "    fig.set_figheight(10)\n",
        "    fig.set_figwidth(20)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JvN7OM_LbwD"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "IPR195xkjqzC",
        "outputId": "78065525-24d8-4347-af9e-8437ac837446"
      },
      "source": [
        "dataaw = Dropdown(options = list(dfs.keys())[1:])\n",
        "\n",
        "@interact(file = dataaw)\n",
        "def asdasd(file):  \n",
        "    emmisions=dict()\n",
        "    c_var='area'\n",
        "    try:\n",
        "        emmisions_keys=set(list(dfs[file]['area']))\n",
        "    except:\n",
        "        c_var='country'\n",
        "        emmisions_keys=set(list(dfs[file][c_var]))\n",
        "    for i in emmisions_keys:\n",
        "        emmisions[i]=list(set(list(dfs[file][dfs[file][c_var]==i]['short_names'])))\n",
        "\n",
        "#     print(emmisions)\n",
        "    countryW = Dropdown(options = emmisions.keys())\n",
        "    typee = Dropdown()\n",
        "\n",
        "    def update_typee_options(*args): \n",
        "        typee.options = emmisions[countryW.value]\n",
        "    typee.observe(update_typee_options) \n",
        "\n",
        "    @interact(country = countryW, emm_type = typee)\n",
        "    def print_city(country, emm_type):\n",
        "        print('Emmision Type Summary:')\n",
        "        print(dfs[sheets[0]][dfs[sheets[0]][list(dfs[sheets[0]].columns)[0]]==emm_type].transpose())\n",
        "        try:\n",
        "            print('\\nFileName: '+file)\n",
        "            print('Country:' +country+'\\nType: '+emm_type+'\\n')\n",
        "            ts=dfs[file][(dfs[file][c_var]==country)&(dfs[file]['short_names']==emm_type)].transpose()\n",
        "            ts=ts[list(ts.index).index(\"short_names\")+1:]\n",
        "            ts['year']=ts.index\n",
        "            ts.reset_index(drop=True, inplace=True)\n",
        "            ts.columns=['value', 'year']\n",
        "            ts.dropna(subset = [\"value\"], inplace=True)\n",
        "            ts['year']=[int(i) for i in ts['year']]\n",
        "            tens_lin_reg(ts['year'], ts['value'])\n",
        "        except:\n",
        "            print('TimeSeries Data Inconsistent!')\n",
        "#             return False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7becefb4743a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataaw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataaw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masdasd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0memmisions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dropdown' is not defined"
          ]
        }
      ]
    }
  ]
}